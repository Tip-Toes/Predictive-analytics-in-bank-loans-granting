{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from plot_metric.functions import BinaryClassification\n",
    "from pylab import rcParams\n",
    "from tensorflow_core.python.keras.wrappers.scikit_learn import KerasClassifier\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from sklearn.metrics import make_scorer, recall_score,precision_score, confusion_matrix,classification_report,accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data= pd.read_csv('../logis_norm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split data en X et Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data1= data.copy()\n",
    "X= data1.drop('Loan Status', axis=1)\n",
    "Y= data1['Loan Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train , X_test , Y_train , Y_test = train_test_split(X, Y, test_size = 0.30, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## ANN Model\n",
    "def create_model(optimizer):\n",
    "    model = Sequential()\n",
    "\t#Layer 1\n",
    "    model.add(Dense(20, activation='relu', input_dim=20))\n",
    "    model.add(Dropout(0.3))\n",
    "    #Layer 2\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(units=1,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=50)\n",
    "ANN = KerasClassifier(build_fn=create_model)\n",
    "\n",
    "params={'optimizer' : ['adam', 'rmsprop'],\n",
    "        'batch_size': [128, 256, 512, 800]}\n",
    "\n",
    "\n",
    "##accuracy\n",
    "grid_search_acc= GridSearchCV(estimator=ANN, param_grid= params, scoring='accuracy',cv=5, n_jobs=-1)\n",
    "grid_search_acc= grid_search_acc.fit(X_train, Y_train, epochs=400, validation_data=(X_test, Y_test), callbacks=[early_stop])\n",
    "y_predict= grid_search_acc.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## evaluating the model\n",
    "print(\"----------\")\n",
    "print('best parameters: ', grid_search_acc.best_params_)\n",
    "print(\"----------\")\n",
    "print(confusion_matrix(Y_test,y_predict))\n",
    "print(\"----------\")\n",
    "print(classification_report(Y_test, y_predict))\n",
    "print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ROC\n",
    "y_predict_proba = grid_search_acc.best_estimator_.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Visualisation with plot_metric\n",
    "bc = BinaryClassification(Y_test, y_predict_proba, labels=[1, 0])\n",
    "# Figures\n",
    "plt.figure(figsize=(7,6))\n",
    "bc.plot_roc_curve()\n",
    "plt.title('Neural Network')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recall\n",
    "rec_avg=make_scorer(recall_score, average='macro')\n",
    "grid_search_rec= GridSearchCV(estimator=ANN, param_grid= params, scoring=rec_avg,cv=5, n_jobs=-1)\n",
    "grid_search_rec= grid_search_rec.fit(X_train, Y_train, epochs=400, validation_data=(X_test, Y_test), callbacks=[early_stop])\n",
    "y_predict= grid_search_rec.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluating the model\n",
    "print(\"----------\")\n",
    "print('best parameters: ', grid_search_rec.best_params_)\n",
    "print(\"----------\")\n",
    "print(confusion_matrix(Y_test,y_predict))\n",
    "print(\"----------\")\n",
    "print(classification_report(Y_test, y_predict))\n",
    "print(\"----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## precision\n",
    "prec_avg=make_scorer(precision_score, average='macro')\n",
    "grid_search_prec_avg= GridSearchCV(estimator=ANN, param_grid= params, scoring=prec_avg,cv=5, n_jobs=-1)\n",
    "grid_search_prec_avg= grid_search_prec_avg.fit(X_train, Y_train)\n",
    "y_predict= grid_search_prec_avg.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluating the model\n",
    "print(\"----------\")\n",
    "print('best parameters: ', grid_search_prec_avg.best_params_)\n",
    "print(\"----------\")\n",
    "print(confusion_matrix(Y_test,y_predict))\n",
    "print(\"----------\")\n",
    "print(classification_report(Y_test, y_predict))\n",
    "print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2, titre):\n",
    "    # Get Test Scores Mean and std for each grid search\n",
    "    scores_mean = cv_results['mean_test_score']\n",
    "    scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    # Plot Grid search scores\n",
    "    _, ax = plt.subplots(1,1)\n",
    "\n",
    "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
    "    for idx, val in enumerate(grid_param_2):\n",
    "        #ax.set_ylim([0.74,0.83])\n",
    "        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))\n",
    "\n",
    "    ax.set_title(titre, fontsize=18, fontweight='bold')\n",
    "    ax.set_xlabel(name_param_1, fontsize=14)\n",
    "    ax.set_ylabel(titre+'[CV Avg Score]', fontsize=14)\n",
    "    ax.legend(loc=\"best\", fontsize=15)\n",
    "    ax.grid('on')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling Method \n",
    "plot_grid_search(grid_search_acc.cv_results_, params['batch_size'], params['optimizer'], 'batch_size', 'optimizer', 'Accuracy')\n",
    "plot_grid_search(grid_search_rec.cv_results_, params['batch_size'], params['optimizer'], 'batch_size', 'optimizer', 'Recall')\n",
    "plot_grid_search(grid_search_prec_avg.cv_results_, params['batch_size'], params['optimizer'], 'batch_size', 'optimizer', 'Precision')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_index_list(param1, param2):\n",
    "    list=[]\n",
    "    for p1 in param1:\n",
    "        for p2 in param2:\n",
    "            list.append('['+str(p1)+', '+str(p2)+']')\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index= return_index_list(params['optimizer'], params['batch_size'])\n",
    "\n",
    "df= pd.DataFrame({  'Accuracy': grid_search_acc.cv_results_['mean_test_score'],\n",
    "                    'Precision(Avg)': grid_search_prec_avg.cv_results_['mean_test_score'],\n",
    "                    'Recall(Avg)': grid_search_rec.cv_results_['mean_test_score'],},\n",
    "                 index=index)\n",
    "\n",
    "## to excel\n",
    "writer = pd.ExcelWriter('Neural_Network.xlsx')\n",
    "df.to_excel(writer, 'Neural_Network')\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grid_search_acc.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for overfiting\n",
    "model_ov= create_model(\"rmsprop\")\n",
    "history= model_ov.fit(x=X_train, y=Y_train, epochs=300, validation_data=(X_test, Y_test), verbose=1, callbacks=[early_stop])\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
