{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from plot_metric.functions import BinaryClassification\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import make_scorer, recall_score,precision_score, confusion_matrix,classification_report,accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data= pd.read_csv('../logis_norm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### split data en X et Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data1= data.copy()\n",
    "X= data1.drop('Loan Status', axis=1)\n",
    "Y= data1['Loan Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train , X_test , Y_train , Y_test = train_test_split(X, Y, test_size = 0.20, random_state = 123, stratify=Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([41020, 41021]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---SVM---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = {'C': [0.1, 1, 10, 100],  \n",
    "          'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}  \n",
    "##accuracy\n",
    "grid_search_acc= GridSearchCV(estimator=SVC(probability=True), param_grid= params, scoring='accuracy',cv=5, n_jobs=-1)\n",
    "grid_search_acc= grid_search_acc.fit(X_train, Y_train)\n",
    "y_predict= grid_search_acc.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## evaluating the model\n",
    "print(\"----------\")\n",
    "print('best parameters: ', grid_search_acc.best_params_)\n",
    "print(\"----------\")\n",
    "print(confusion_matrix(Y_test,y_predict))\n",
    "print(\"----------\")\n",
    "print(classification_report(Y_test, y_predict))\n",
    "print(\"----------\")\n",
    "\n",
    "\n",
    "##ROC\n",
    "y_predict_proba = grid_search_acc.best_estimator_.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Visualisation with plot_metric\n",
    "bc = BinaryClassification(Y_test, y_predict_proba, labels=[1, 0])\n",
    "# Figures\n",
    "plt.figure(figsize=(7,6))\n",
    "bc.plot_roc_curve()\n",
    "plt.title('SVM_logistic_Smote')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Recall\n",
    "rec_avg=make_scorer(recall_score, average='macro')\n",
    "grid_search_rec= GridSearchCV(estimator=SVC(probability=True), param_grid= params, scoring=rec_avg,cv=5, n_jobs=-1)\n",
    "grid_search_rec= grid_search_rec.fit(X_train, Y_train)\n",
    "y_predict= grid_search_rec.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## evaluating the model\n",
    "print(\"----------\")\n",
    "print('best parameters: ', grid_search_rec.best_params_)\n",
    "print(\"----------\")\n",
    "print(confusion_matrix(Y_test,y_predict))\n",
    "print(\"----------\")\n",
    "print(classification_report(Y_test, y_predict))\n",
    "print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## precision\n",
    "prec_avg=make_scorer(precision_score, average='macro')\n",
    "grid_search_prec_avg= GridSearchCV(estimator=SVC(probability=True), param_grid= params, scoring=prec_avg,cv=5, n_jobs=-1)\n",
    "grid_search_prec_avg= grid_search_prec_avg.fit(X_train, Y_train)\n",
    "y_predict= grid_search_prec_avg.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## evaluating the model\n",
    "print(\"----------\")\n",
    "print('best parameters: ', grid_search_prec_avg.best_params_)\n",
    "print(\"----------\")\n",
    "print(confusion_matrix(Y_test,y_predict))\n",
    "print(\"----------\")\n",
    "print(classification_report(Y_test, y_predict))\n",
    "print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2, titre):\n",
    "    # Get Test Scores Mean and std for each grid search\n",
    "    scores_mean = cv_results['mean_test_score']\n",
    "    scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    # Plot Grid search scores\n",
    "    _, ax = plt.subplots(1,1)\n",
    "\n",
    "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
    "    for idx, val in enumerate(grid_param_2):\n",
    "        ax.set_ylim([0.70,0.86])\n",
    "        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))\n",
    "\n",
    "    ax.set_title(titre, fontsize=18, fontweight='bold')\n",
    "    ax.set_xlabel(name_param_1, fontsize=14)\n",
    "    ax.set_ylabel(titre+'[CV Avg Score]', fontsize=14)\n",
    "    ax.legend(loc=\"best\", fontsize=15)\n",
    "    ax.grid('on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calling Method \n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "plot_grid_search(grid_search_acc.cv_results_, params['gamma'], params['C'], 'gamma', 'C',  'Accuracy')\n",
    "plot_grid_search(grid_search_rec.cv_results_, params['gamma'], params['C'], 'gamma', 'C', 'Recall')\n",
    "plot_grid_search(grid_search_prec_avg.cv_results_, params['gamma'], params['C'], 'gamma', 'C', 'Precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def return_index_list(param1, param2):\n",
    "    list=[]\n",
    "    for p1 in param1:\n",
    "        for p2 in param2:\n",
    "            list.append('['+str(p1)+', '+str(p2)+']')\n",
    "    return list\n",
    "index= return_index_list(params['C'], params['gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df= pd.DataFrame({  'Accuracy': grid_search_acc.cv_results_['mean_test_score'],\n",
    "                    'Precision(Avg)': grid_search_prec_avg.cv_results_['mean_test_score'],\n",
    "                    'Recall(Avg)': grid_search_rec.cv_results_['mean_test_score'],},\n",
    "                 index=index)\n",
    "\n",
    "path= \"../../tables_smote.xlsx\"\n",
    "writer = pd.ExcelWriter(path, engine=\"openpyxl\")\n",
    "from openpyxl import load_workbook\n",
    "writer.book= load_workbook(path)\n",
    "df.to_excel(writer, 'SVM_logistic')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
